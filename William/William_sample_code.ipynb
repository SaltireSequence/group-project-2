{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://github.com/hhursev/recipe-scrapers\n",
    "\n",
    "# scraper library dependencies\n",
    "from recipe_scrapers import scrape_me\n",
    "import re\n",
    "\n",
    "# scraping dependencies\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - saving index links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving URL in a variable to pass into the scraper library.\n",
    "scraper = scrape_me('https://www.simplyrecipes.com/index/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of dictionaries containting all of the <a> tag attributes from the defined website. The attribute names are assigned as dictionary keys and the \n",
    "raw_dict = scraper.links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop that iterates through the dictionary\n",
    "result = []\n",
    "for d in raw_dict:\n",
    "    # Iterates through every key in the dictionary\n",
    "    for key, value in d.items():\n",
    "        # If the key is not equal to 'href' and the value doesn't contain ''/recipe' then that pair is skipped.\n",
    "        if key != \"href\" or \"https://www.simplyrecipes.com/recipes/\" not in value:\n",
    "            # Move onto next iteration\n",
    "            continue\n",
    "        txt = ''\n",
    "        # using part of the URL (key value) to determine the key name\n",
    "        for i in re.findall(\"recipes/.*\", value):\n",
    "            txt += i\n",
    "            title = txt.split('/')[1]\n",
    "            # appending each key and value to a nested dictionary in a list.\n",
    "\n",
    "            result.append({title: value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - code to scrape a recipe's URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: we can probably implement this code into a for loop and iterate through whatever structure our complete URL list sits within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 84.0.4147\n",
      "[WDM] - Get LATEST driver version for 84.0.4147\n",
      "[WDM] - Driver [/Users/williamforsyth/.wdm/drivers/chromedriver/mac64/84.0.4147.30/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "# boilerplate code to setup Browser functionality during scrape.\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting URL for requets\n",
    "resp = requests.get(\"https://www.simplyrecipes.com/recipes/egg_salad_sandwich/\")\n",
    "\n",
    "# Initiating HTML parser, using BeautifulSoup\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# using soup.find to find <div> tags with a class of \"recipe-callout\", that contains each URL recipe\n",
    "div_ = soup.find(\"div\", attrs={\"class\": \"recipe-callout\"})\n",
    "\n",
    "# Firstly finding all H2 classes and setting as the key. Then iterating through every list item class within \"recipe-callout\" and splitting the text as the value for that key\n",
    "recipes = {\"_\".join(div_.find(\"h2\").text.split()):\n",
    "               [x.text for x in div_.findAll(\"li\", attrs={\"class\": \"ingredient\"})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Quick_and_Easy_Egg_Salad_Sandwich_Recipe': ['1 hard-boiled large egg, peeled and chopped (See Recipe Note)',\n",
       "  '1-2 tablespoons mayonnaise (to taste)',\n",
       "  '2 tablespoons chopped celery',\n",
       "  '1 tablespoon chopped green onion or chives',\n",
       "  'A pinch of curry powder',\n",
       "  'Salt and pepper (to taste)',\n",
       "  'Lettuce',\n",
       "  '2 slices white, wheat, multigrain, or rye bread, toasted or plain']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
